{
    "valid_concepts": {
        "Machine Learning Fundamentals": [
            "regression", "classification", "clustering", "dimensionality reduction",
            "feature selection", "cross validation", "bias variance tradeoff",
            "overfitting", "underfitting", "ensemble methods", "decision trees",
            "random forest", "svm", "bagging", "boosting", "voting classifiers"
        ],
        "Feature Engineering": [
            "data normalization", "min-max scaling", "standard scaling", "robust scaling",
            "missing data imputation", "mean imputation", "median imputation", "knn imputation",
            "one-hot encoding", "label encoding", "target encoding", "feature hashing",
            "polynomial features", "interaction terms", "feature crossing",
            "log transformation", "box-cox transformation", "yeo-johnson transformation",
            "feature selection", "dimensionality reduction", "feature importance",
            "preprocessing", "data augmentation"
        ],
        "Supervised Learning": [
            "linear regression", "logistic regression", "ordinary least squares",
            "polynomial regression", "ridge regression", "lasso regression", "elastic net",
            "binary classification", "multiclass classification",
            "svm fundamentals", "kernel tricks", "support vectors", "margin optimization",
            "decision trees", "random forests", "gradient boosting",
            "cross-entropy loss", "regression metrics", "classification metrics"
        ],
        "Unsupervised Learning": [
            "kmeans clustering", "elbow method", "silhouette score",
            "hierarchical clustering", "linkage methods", "dendrogram interpretation",
            "dbscan", "density-based clustering", "noise handling",
            "gaussian mixture models", "em algorithm", "cluster validation",
            "pca", "eigenvalues", "explained variance",
            "tsne", "manifold learning", "dimensionality reduction",
            "autoencoders", "latent space", "reconstruction loss"
        ],
        "Neural Networks and Deep Learning": [
            "perceptrons", "activation functions", "neural architecture",
            "backpropagation", "chain rule", "gradient computation",
            "cnn", "rnn", "lstm", "gru", "transformer",
            "attention mechanism", "self-attention", "positional encoding",
            "bert", "gpt", "transfer learning", "fine-tuning",
            "batch normalization", "dropout", "regularization",
            "vanishing gradients", "exploding gradients", "initialization"
        ],
        "Deep Learning Applications": [
            "computer vision", "image classification", "object detection",
            "semantic segmentation", "region proposal", "iou metrics",
            "gan architecture", "generator", "discriminator",
            "sequence models", "natural language processing",
            "speech recognition", "time series forecasting",
            "recommendation systems", "anomaly detection"
        ],
        "Large Language Models": [
            "llm architecture", "transformer models", "attention mechanisms",
            "prompt engineering", "few-shot learning", "zero-shot learning",
            "tokenization", "embeddings", "bert variants", "gpt variants",
            "prompt tuning", "instruction tuning", "chain of thought",
            "model alignment", "model distillation", "efficient inference"
        ],
        "Reinforcement Learning": [
            "mdp fundamentals", "state space", "action space",
            "value iteration", "policy iteration", "bellman equation",
            "q learning", "dqn", "experience replay",
            "policy gradients", "reinforce", "advantage estimation",
            "ppo", "actor critic", "exploration exploitation",
            "multi-agent rl", "reward shaping", "curriculum learning"
        ],
        "Model Optimization": [
            "gradient descent variants", "learning rate scheduling",
            "momentum", "adam", "rmsprop", "adagrad",
            "batch size selection", "early stopping",
            "hyperparameter tuning", "grid search", "random search",
            "bayesian optimization", "gaussian processes",
            "cross validation strategies", "nested cross validation"
        ],
        "Model Evaluation": [
            "validation strategies", "k-fold cross validation",
            "stratified sampling", "holdout method",
            "classification metrics", "confusion matrix", "precision",
            "recall", "f1 score", "roc curve", "auc",
            "regression metrics", "rmse", "mae", "r squared",
            "learning curves", "validation curves"
        ],
        "MLOps and Production": [
            "model deployment", "model monitoring", "model versioning",
            "ci/cd pipelines", "feature stores", "model registry",
            "model serving", "a/b testing", "model drift detection",
            "data drift detection", "mlflow", "kubeflow",
            "distributed training", "mixed precision training",
            "model parallelism", "containerization", "orchestration"
        ]
    },
    "ollama_config": {
        "url": "http://localhost:11434/api/generate",
        "default_model": "qwen2.5:latest",
        "generation_params": {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "num_ctx": 8192,
            "num_predict": 4096,
            "repeat_penalty": 1.1,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0,
            "tfs_z": 1.0,
            "mirostat": 2,
            "mirostat_tau": 5.0,
            "mirostat_eta": 0.1,
            "seed": 42
        }
    },
    "question_gen_config": {
        "max_concurrent_queries": 3,
        "max_retries": 3,
        "batch_size": 5,
        "similarity_threshold": 0.85,
        "question_format": "You are an expert in machine learning and deep learning, particularly in {concept}.\nGenerate exactly {num_questions} multiple choice questions.{existing_context}\n\nEach question MUST follow this EXACT format:\n\nQ1. [Your question text here]\nA) [Option A]\nB) [Option B]\nC) [Option C]\nD) [Option D]\nCorrect: [A, B, C, or D]\nExplanation: [Your detailed explanation]\n\nRequirements for each question:\n1. Question must be clear and technical\n2. All four options (A, B, C, D) must be provided\n3. Only ONE option should be correct\n4. Correct answer must be specified as A, B, C, or D\n5. Explanation must be detailed and technical\n6. Each question should focus on a different aspect of {concept}\n7. Make sure all questions are unique\n\nExample format:\nQ1. In the context of neural networks, what is the primary purpose of the ReLU (Rectified Linear Unit) activation function?\nA) To normalize input values between 0 and 1\nB) To introduce non-linearity and handle vanishing gradients\nC) To reduce model complexity\nD) To regularize the network weights\nCorrect: B\nExplanation: ReLU (f(x) = max(0,x)) is widely used because it effectively handles the vanishing gradient problem present in earlier activation functions like sigmoid. It introduces non-linearity while being computationally efficient, as it simply outputs the input for positive values and zero for negative values. This helps maintain strong gradients during backpropagation, enabling faster training of deep networks. Unlike sigmoid/tanh, ReLU doesn't saturate for positive values, allowing for better gradient flow.\n\nQ2. [Next question follows the same format]\n\nRemember: Generate EXACTLY {num_questions} complete questions following this format precisely."
    }
}
